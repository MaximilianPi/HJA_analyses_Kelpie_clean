#!/bin/sh
#SBATCH -t 48:00:00 # 48 hours
#SBATCH -p compute-24-96 # -p for 'partition name', where partition is the queue
	# compute-24-96 # 24 hrs default, 7 d max, 24 cores max, 4 GB per core, 96 GB total
	# compute-16-64
	# compute-24-128
	# ib-28-128
	# ib-24-96
	# hmem-754 # 24 hrs default, 7 d max, 24 cores max, 32GB per core, 754 GB total
	# hmem-691 # 24 hrs default, 7 d max, 24 cores max, 32GB per core, 691 GB total
	# see below for gpu
#SBATCH --mem 4G # max mem, default 4G, diff units can be specified using [K|M|G|T], # probably set ~42 GB for future Kelpie jobs because 2003/4/5 job took 39.6G, max mem on compute nodes is 96 GB
#SBATCH -o s3dnld.out # 10 chars
#SBATCH -e s3dnld.err # 10 chars
#SBATCH --mail-type=BEGIN # options NONE, BEGIN, END, FAIL, ALL
#SBATCH --mail-type=END # options NONE, BEGIN, END, FAIL, ALL
#SBATCH --mail-user=dougwyu@mac.com #sends email to me
#SBATCH --ntasks=16 # number of slots == number of cores. 24 is the max per node
#SBATCH --job-name=s3dnld # job name, 8 chars

module add s3cmd/2.2.0 # makes s3cmd available

cd ~/_Oregon/HJAdryad || exit

# s3cmd get s3://BUCKET/OBJECT LOCAL_FILE
s3cmd get s3://amazon-oregon-douglasyu/2019Sep_shotgun_2.trimmeddata.tar 2019Sep_shotgun_2.trimmeddata.tar
s3cmd ls --list-md5 s3://amazon-oregon-douglasyu > amazon-oregon-douglasyu_20220202.md5
md5sum 2019Sep_shotgun_2.trimmeddata.tar > 2019Sep_shotgun_2.trimmeddata.tar.md5
